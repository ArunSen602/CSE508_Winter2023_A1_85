{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Creating unigram index"
      ],
      "metadata": {
        "id": "PqscZTqn-rj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pk\n",
        "import os\n",
        "import nltk"
      ],
      "metadata": {
        "id": "cLBe4xCYMi01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unigram_Index:\n",
        "  def __init__(self):\n",
        "    self.index_dict=dict()\n",
        "\n",
        "  def make_tok_list(self):\n",
        "    os.mkdir('temp_dir1')\n",
        "    with ZipFile(\"/content/CSE508_Winter2023_Dataset_1.zip\", 'r') as zObject:    # Extracting all the members of the zip \n",
        "    # into a specific location.\n",
        "      zObject.extractall(path=\"temp_dir1\")\n",
        "    dir_path='/content/temp_dir/content/temp_dir/CSE508_Winter2023_Dataset'\n",
        "    files_list=os.listdir(dir_path)\n",
        "    file_tok_list=list()\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      st=fobj.read()\n",
        "      word_toks=nltk.word_tokenize(st)\n",
        "      self.token_list.append(word_toks)\n",
        "      fobj.close()\n",
        "\n",
        "  def make_unigram(self):\n",
        "    os.mkdir('temp_dir1')\n",
        "    with ZipFile(\"/content/CSE508_Winter2023_Dataset_1.zip\", 'r') as zObject:    # Extracting all the members of the zip \n",
        "    # into a specific location.\n",
        "      zObject.extractall(path=\"temp_dir1\")\n",
        "    dir_path='/content/temp_dir1/content/temp_dir/CSE508_Winter2023_Dataset'\n",
        "    files_list=os.listdir(dir_path)\n",
        "    file_tok_list=list()\n",
        "    for fl in files_list:\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      st=fobj.read(sep=',')\n",
        "      word_toks=nltk.unigrams(st)\n",
        "      self.token_list.append(word_toks)\n",
        "      fobj.close()\n",
        "\n",
        "  def make_index(self):\n",
        "    fin_token_list=set()\n",
        "    #index_dict=dict()\n",
        "    self.make_unigram()\n",
        "    for toks in self.token_list:\n",
        "      fin_token_list=fin_token_list.union(set(toks))\n",
        "    fin_token_list=list(fin_token_list)\n",
        "    for tok in fin_token_list:\n",
        "      self.index_dict[tok]=dict()\n",
        "    for doc in range(len(self.token_list)):\n",
        "      for ug in self.token_list[doc]:\n",
        "          self.index_dict[ug].append(doc)\n",
        "      #index_dict[self.token_list[doc][pos]]\n",
        "    return self.index_dict\n",
        "\n",
        "    def and_operator(self,s1,s2):\n",
        "      if s1 in self.index_dict and s2 in self.index_dict:\n",
        "        set_s1 = set(self.index_dict[s1])\n",
        "        set_s2 = set(self.index_dict[s2])\n",
        "        return set_s1.intersection(set_s2)\n",
        "\n",
        "    def or_operator(self,s1,s2):\n",
        "      if s1 in self.index_dict and s2 in self.index_dict:\n",
        "        set_s1 = set(self.index_dict[s1])\n",
        "        set_s2 = set(self.index_dict[s2])\n",
        "        return set_s1.union(set_s2)\n",
        "    \n"
      ],
      "metadata": {
        "id": "OxsLVBfFEEpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l--cntthGaWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}