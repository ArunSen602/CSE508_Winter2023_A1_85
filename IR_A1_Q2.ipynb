{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MO8hVl4RWvt5",
        "outputId": "b97bc812-6c55-43f4-8c77-b3673dc8394c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c9c6f3c-6557-4e8a-b0ef-a302116be9a0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c9c6f3c-6557-4e8a-b0ef-a302116be9a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CSE508_Winter2023_Dataset3_preprocessed.zip to CSE508_Winter2023_Dataset3_preprocessed.zip\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=list(stopwords.words(\"english\"))\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "uploads=files.upload()                           # preprocessed zipped collection of document files is expected to be uploaded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Inverted_Index:\n",
        "  def __init__(self):\n",
        "    self.index_dict = dict()                      # dictionary for storing unigram inverted index\n",
        "    self.token_list = list()                      # list to store all of the words present in the documents\n",
        "    self.doc_count = 0                            # variable to store the total count of documents\n",
        "    self.word_freq = dict()                       # dictionary for storing the count of indices stored for each word in the unigram inverted index\n",
        "    self.all_doc_ids = []                         # list to store all document id numbers\n",
        "    self.doc_id_map = dict()                      # dictionary that maps document_id to document_name\n",
        "\n",
        "\n",
        "  def make_tok_list(self):\n",
        "    os.mkdir('temp_dir')\n",
        "    with ZipFile(\"/content/CSE508_Winter2023_Dataset3_preprocessed.zip\", 'r') as zObject:    \n",
        "      zObject.extractall(path=\"temp_dir\")                                        # Extracting all the files from the zip into a specific location\n",
        "    dir_path='/content/temp_dir/content/temp_dir/CSE508_Winter2023_Dataset'\n",
        "    files_list=os.listdir(dir_path)                                              # getting the files in a list          \n",
        "    i = 0\n",
        "    for fl in files_list:\n",
        "      self.doc_id_map[i] = fl\n",
        "      self.all_doc_ids.append(i)                        # adding the current file's document_id to the list all_doc_ids\n",
        "      i += 1\n",
        "      self.doc_count += 1\n",
        "      fl_path=os.path.join(dir_path,fl)\n",
        "      fobj=open(fl_path,'r')\n",
        "      text=fobj.read()\n",
        "      word_toks = text.split(\",\")                       # the words in the file are seperated by comma, to extract them spliting wrt \",\"\n",
        "      self.token_list.append(word_toks)                 # then adding those words in the token_list, which is collection of all present tokens\n",
        "      fobj.close()\n",
        "\n",
        "\n",
        "  def make_unigram(self):\n",
        "    unique_token_list=set()                                       # stores unique tokens present in token_list\n",
        "    self.make_tok_list()\n",
        "    for toks in self.token_list:\n",
        "      unique_token_list = unique_token_list.union(set(toks))      # getting the unique token from tokens present in token_list using set union\n",
        "    unique_token_list = list(unique_token_list)                   \n",
        "    for tok in unique_token_list:\n",
        "      self.index_dict[tok] = []                                   # initializing each inverted index as empty list\n",
        "      self.word_freq[tok] = 0                                     # initializing the length of each inverted index as zero\n",
        "    for doc in range(self.doc_count):                             # generating the unigram inverted index\n",
        "      for tok in self.token_list[doc]:\n",
        "        if doc not in self.index_dict[tok]:\n",
        "          self.index_dict[tok].append(doc)\n",
        "          self.word_freq[tok] = self.word_freq[tok] + 1\n",
        "    for tok in unique_token_list:\n",
        "      self.index_dict[tok].sort()                                 # sorting the document_ids present in each inverted index in increasing order\n",
        "\n",
        "\n",
        "  def t1_AND_t2(self,doc_list1,doc_list2,l1,l2):          # takes the inverted index list of two operand tokens and their lengths as input\n",
        "    if l1 == 0 and l2 ==0:                                # if none of the operands present in the index_dict the return empty list\n",
        "      return [],0\n",
        "    i, j, comparisons = 0, 0, 0                           # the variable comparisons stores the number of comparisons made for this task\n",
        "    docs = []\n",
        "    while i<l1 and j<l2:                                      \n",
        "      comparisons += 1\n",
        "      if doc_list1[i] == doc_list2[j]:                    # add the document_ids present in both doc_list1 and doc_list2 in a new list 'docs'\n",
        "        docs.append(doc_list1[i])\n",
        "        i += 1\n",
        "        j += 1\n",
        "      elif doc_list1[i] < doc_list2[j]:\n",
        "        i += 1\n",
        "      else:\n",
        "        j += 1\n",
        "    return docs,comparisons                               # return the desired list 'docs', and comparisons_count\n",
        "\n",
        "\n",
        "  def t1_OR_t2(self,doc_list1,doc_list2,l1,l2):           # takes the inverted index list of two operand tokens and their lengths as input\n",
        "    if l1!=0 and l2!=0:                                   # condition to check if both of the tokens are present in the inverted index\n",
        "      i, j, comparisons = 0, 0, 0                         # the variable comparisons stores the number of comparisons made for this task\n",
        "      docs = []\n",
        "      while i<l1 and j<l2:                                # merging doc_list1 and doc_list2 into resultant list 'docs' \n",
        "        comparisons+=1                                    # it is considered that doc_list1 and doc_list2 may have many common elements\n",
        "        if doc_list1[i]<doc_list2[j]:\n",
        "          docs.append(doc_list1[i])\n",
        "          i=i+1\n",
        "        elif doc_list1[i]>doc_list2[j]:              \n",
        "          docs.append(doc_list2[j])\n",
        "          j=j+1\n",
        "        else:\n",
        "          docs.append(doc_list1[i])\n",
        "          i=i+1\n",
        "          j=j+1\n",
        "      docs = docs + doc_list1[i:] + doc_list2[j:]        \n",
        "      return docs,comparisons                             # return the desired list 'docs', and comparisons_count\n",
        "    elif l1!=0:                                           # condition to check if only the first token is present in the inverted index\n",
        "      return doc_list1,0                                  # simply return the inverted index for first token\n",
        "    elif l2!=0:                                           # condition to check if only the second token is present in the inverted index\n",
        "      return doc_list2,0                                  # simply return the inverted index for second token\n",
        "    else:                                                 # if none of the tokens are present in the dictionary the return empty list\n",
        "      return [],0\n",
        "\n",
        "\n",
        "  def t1_AND_NOT_t2(self,doc_list1,doc_list2,l1,l2):      # takes the inverted index list of two operand tokens and their lengths as input\n",
        "    if l1 == 0:                                           # condition to check if the first token is not present in the index_dict\n",
        "      return [],0                                         # if so then just return empty list\n",
        "    docs = []\n",
        "    i, j, comparisons = 0, 0, 0                           # the variable comparisons stores the number of comparisons made for this task\n",
        "    while i<l1 and j<l2:                                  # run the loop untill at least one of the index-list donesn't exhaust\n",
        "      comparisons += 1\n",
        "      if doc_list1[i] == doc_list2[j]:\n",
        "        i += 1\n",
        "        j += 1\n",
        "      elif doc_list1[i] < doc_list2[j]:\n",
        "        docs.append(doc_list1[i])                         # adding the doc-id's which are in doc_list1 but not in doc_list2 in a new list 'docs'\n",
        "        i += 1\n",
        "      else:\n",
        "        j += 1\n",
        "    docs = docs + doc_list1[i:]\n",
        "    return docs,comparisons                              # return the desired list 'docs', and comparisons_count\n",
        "\n",
        "\n",
        "  def t1_OR_NOT_t2(self,doc_list1,doc_list2,l1,l2):       # takes the inverted index list of two operand tokens and their lengths as input\n",
        "    if l2 == 0:\n",
        "      return self.all_doc_ids,0\n",
        "    docs = self.all_doc_ids.copy()                        # copy all possible document-ids in the result-list 'docs'\n",
        "    comparisons = 0                                  # the variable comparisons stores the number of comparisons made for this task\n",
        "    for id in self.all_doc_ids:\n",
        "      comparisons += 1\n",
        "      if id in doc_list2:\n",
        "        docs.remove(id)                                   # if a doc-id is present in doc_list2 then remove it from the result-list 'docs'\n",
        "      if id >= doc_list2[-1]:                             # if the current doc-id is more than the last doc-id of doc_list2 (sorted list)\n",
        "        break                                             # then get out of the loop to avoid unnecessary steps\n",
        "    return docs,comparisons                      # return the document_ids present in either doc_list1 or not in doc_list2, and comparisons_count\n",
        "\n",
        "\n",
        "  def preprocess_query(self,query):                              # function for preprocessing the input query string                   \n",
        "    query = query.lower()                                        # converting the texts of query string into lowercase\n",
        "    words = re.sub('[^\\w\\s]',' ',query)                          # removing non-alphanumeric and non-space characters from the query string\n",
        "    tokens = nltk.word_tokenize(words)                           # tokenizing the modified query string\n",
        "    new_tokens = [t for t in tokens if t not in stop_words]      # removing stopwords from the tokens obtained in last step\n",
        "    words_list = [t for t in new_tokens if t!=' ']               # removing all blank-space tokens\n",
        "    return words_list                                            # returning a list of filtered tokens\n",
        "\n",
        "\n",
        "  def query_processing(self,query_sent,operations):\n",
        "    tokens = self.preprocess_query(query_sent)                   # a list of tokens will be returned here after preprocessing the whole string query\n",
        "    ops=operations.split(\",\")                                    # the comma seperated boolean operators received as i/p are stored in list 'ops'\n",
        "    processed_query = \"\"                                  # will store a string containing all operands and operators in b/w them in proper sequence\n",
        "    for i in range(len(tokens)):\n",
        "      processed_query = processed_query + tokens[i] + \" \"\n",
        "      if i != len(tokens)-1:\n",
        "        processed_query = processed_query + ops[i] + \" \"\n",
        "    pos=0\n",
        "    func_dict={\"AND\":self.t1_AND_t2,\"OR\":self.t1_OR_t2,\"OR NOT\":self.t1_OR_NOT_t2,\"AND NOT\":self.t1_AND_NOT_t2}            # dictionary containing \n",
        "    docs = []                                                            # boolean operators as keys and their respective function names as values\n",
        "    comparisons, i, j = 0,0,0 \n",
        "    while i < len(tokens):\n",
        "      doc_list1, doc_list2 = [], []\n",
        "      l1, l2 = 0, 0\n",
        "      if pos==0:                                                         # the first boolean operator will be applied on first two words\n",
        "        if tokens[0] in self.index_dict:                                 # from the preprocessed i/p sequence 'tokens'\n",
        "          doc_list1 = self.index_dict[tokens[0]]\n",
        "          l1 = self.word_freq[tokens[0]]\n",
        "        if tokens[1] in self.index_dict:\n",
        "          doc_list2 = self.index_dict[tokens[1]]\n",
        "          l2 = self.word_freq[tokens[1]]\n",
        "        docs, count = func_dict[ops[j]](doc_list1,doc_list2,l1,l2)\n",
        "        pos += 1\n",
        "        i += 2\n",
        "      else:                                         # from second boolean operator and onwards \n",
        "        if tokens[i] in self.index_dict:            # the 1st operand will be the resultant list of doc-ids obtained from previous boolean operation\n",
        "          doc_list2 = self.index_dict[tokens[i]]    # and the 2nd operand will be next available word from the preprocessed i/p sequence 'tokens'           \n",
        "          l2 = self.word_freq[tokens[i]]\n",
        "        docs, count = func_dict[ops[j]](docs,doc_list2,len(docs),l2)\n",
        "        i += 1\n",
        "      j += 1\n",
        "      comparisons += count                          # comaparisons required for performing each operators will be added to get total\n",
        "                                                    # count of comparisons required for the entire complex query\n",
        "    return processed_query,docs,comparisons         # returning the desired outputs   \n",
        "\n",
        "\n",
        "  def input_output(self):                                          # funtion to take the inputs in given format, call the query_processing method \n",
        "    n = int(input(\"Enter number of queries to execute : \"))        # and then print the output in desired format\n",
        "    query = [None for i in range(n)]\n",
        "    op = [None for i in range(n)]\n",
        "    for i in range(n):\n",
        "      query[i] = input(\"Enter Input sequence : \")\n",
        "      op[i] = input(\"Enter Operations separated by comma : \")\n",
        "    for i in range(n):\n",
        "      processed_query, doc_ids, comparisons = self.query_processing(query[i],op[i])\n",
        "      doc_count = len(doc_ids)\n",
        "      doc_names = [self.doc_id_map[id] for id in doc_ids]\n",
        "      doc_names.sort()\n",
        "      names = \"\"\n",
        "      for j in range(doc_count):\n",
        "        if j==doc_count-1:\n",
        "          names = names + doc_names[j] + \".txt\"\n",
        "        else:\n",
        "          names = names + doc_names[j] + \".txt\" + \", \"\n",
        "      print(\"\\n\\nQuery {}: {}\".format(i+1,processed_query))\n",
        "      print(\"Number of documents retrieved for query {}: {}\".format(i+1,doc_count))\n",
        "      print(\"Names of the documents retrieved for query {}: {}\".format(i+1,names))\n",
        "      print(\"Number of comparisons required for query {}: {}\".format(i+1,comparisons))\n",
        "      \n"
      ],
      "metadata": {
        "id": "kIYZd3yR_3qb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = Inverted_Index()                  # create object of the Inverted_Index class\n",
        "obj.make_unigram()                      # generate the unigram inverted index for this instance obj"
      ],
      "metadata": {
        "id": "2_AUb4vK_6ea"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl                                \n",
        "f1=open(\"Unigram_inverted_index.pkl\",'wb')\n",
        "pkl.dump(obj,f1)                                 # store this class object obj as pkl file for future use\n",
        "f1.close()"
      ],
      "metadata": {
        "id": "dZVClKTwR4TB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('Unigram_inverted_index.pkl')     # downloading the pl=kl file for storing locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GyAu48dFTOol",
        "outputId": "0029f134-b62f-4f94-a5f6-95d9c5c69eb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fb0509cd-cc4b-429c-96d5-c46256d4fc77\", \"Unigram_inverted_index.pkl\", 1779127)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl       \n",
        "uploads=files.upload()                            \n",
        "f2=open(\"Unigram_inverted_index.pkl\",'rb')\n",
        "uii=pkl.load(f2)                                  # load the pkl file stored earlier\n",
        "f2.close() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fzzLwfv7SCJW",
        "outputId": "ff6533ee-0942-46db-d870-b83a88f9995d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a840854a-be3e-4b98-844a-33301e4222ba\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a840854a-be3e-4b98-844a-33301e4222ba\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Unigram_inverted_index.pkl to Unigram_inverted_index.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uii.input_output()          # call the input_output method using pkl object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjBI5tL2IHCX",
        "outputId": "df04213a-9dd8-424a-958a-1df341ffe4b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries to execute : 2\n",
            "Enter Input sequence : Body in Shapes, is well !!!\n",
            "Enter Operations separated by comma : AND,OR\n",
            "Enter Input sequence : Profiles for Match.\n",
            "Enter Operations separated by comma : AND NOT\n",
            "\n",
            "\n",
            "Query 1: body AND shapes OR well \n",
            "Number of documents retrieved for query 1: 170\n",
            "Names of the documents retrieved for query 1: cranfield0001.txt, cranfield0008.txt, cranfield0012.txt, cranfield0014.txt, cranfield0020.txt, cranfield0022.txt, cranfield0044.txt, cranfield0059.txt, cranfield0063.txt, cranfield0077.txt, cranfield0079.txt, cranfield0089.txt, cranfield0094.txt, cranfield0096.txt, cranfield0115.txt, cranfield0122.txt, cranfield0126.txt, cranfield0146.txt, cranfield0152.txt, cranfield0160.txt, cranfield0165.txt, cranfield0166.txt, cranfield0168.txt, cranfield0173.txt, cranfield0177.txt, cranfield0186.txt, cranfield0187.txt, cranfield0196.txt, cranfield0202.txt, cranfield0213.txt, cranfield0232.txt, cranfield0233.txt, cranfield0257.txt, cranfield0274.txt, cranfield0287.txt, cranfield0289.txt, cranfield0306.txt, cranfield0307.txt, cranfield0330.txt, cranfield0343.txt, cranfield0356.txt, cranfield0363.txt, cranfield0369.txt, cranfield0373.txt, cranfield0376.txt, cranfield0400.txt, cranfield0417.txt, cranfield0433.txt, cranfield0443.txt, cranfield0447.txt, cranfield0453.txt, cranfield0456.txt, cranfield0463.txt, cranfield0469.txt, cranfield0486.txt, cranfield0500.txt, cranfield0521.txt, cranfield0528.txt, cranfield0556.txt, cranfield0572.txt, cranfield0599.txt, cranfield0602.txt, cranfield0606.txt, cranfield0620.txt, cranfield0622.txt, cranfield0626.txt, cranfield0630.txt, cranfield0636.txt, cranfield0637.txt, cranfield0640.txt, cranfield0648.txt, cranfield0686.txt, cranfield0688.txt, cranfield0697.txt, cranfield0703.txt, cranfield0704.txt, cranfield0710.txt, cranfield0711.txt, cranfield0732.txt, cranfield0747.txt, cranfield0758.txt, cranfield0762.txt, cranfield0782.txt, cranfield0794.txt, cranfield0796.txt, cranfield0798.txt, cranfield0801.txt, cranfield0808.txt, cranfield0809.txt, cranfield0814.txt, cranfield0815.txt, cranfield0816.txt, cranfield0825.txt, cranfield0830.txt, cranfield0847.txt, cranfield0850.txt, cranfield0874.txt, cranfield0887.txt, cranfield0890.txt, cranfield0893.txt, cranfield0902.txt, cranfield0903.txt, cranfield0908.txt, cranfield0912.txt, cranfield0926.txt, cranfield0928.txt, cranfield0934.txt, cranfield0947.txt, cranfield0959.txt, cranfield0960.txt, cranfield0962.txt, cranfield0975.txt, cranfield0980.txt, cranfield1002.txt, cranfield1023.txt, cranfield1024.txt, cranfield1026.txt, cranfield1027.txt, cranfield1047.txt, cranfield1052.txt, cranfield1053.txt, cranfield1065.txt, cranfield1074.txt, cranfield1079.txt, cranfield1097.txt, cranfield1104.txt, cranfield1106.txt, cranfield1108.txt, cranfield1110.txt, cranfield1118.txt, cranfield1126.txt, cranfield1139.txt, cranfield1150.txt, cranfield1179.txt, cranfield1185.txt, cranfield1187.txt, cranfield1191.txt, cranfield1195.txt, cranfield1197.txt, cranfield1198.txt, cranfield1201.txt, cranfield1218.txt, cranfield1225.txt, cranfield1229.txt, cranfield1239.txt, cranfield1240.txt, cranfield1241.txt, cranfield1247.txt, cranfield1268.txt, cranfield1269.txt, cranfield1271.txt, cranfield1273.txt, cranfield1274.txt, cranfield1293.txt, cranfield1299.txt, cranfield1301.txt, cranfield1303.txt, cranfield1309.txt, cranfield1310.txt, cranfield1319.txt, cranfield1322.txt, cranfield1332.txt, cranfield1367.txt, cranfield1370.txt, cranfield1371.txt, cranfield1373.txt, cranfield1375.txt, cranfield1377.txt, cranfield1380.txt, cranfield1381.txt\n",
            "Number of comparisons required for query 1: 396\n",
            "\n",
            "\n",
            "Query 2: profiles AND NOT match \n",
            "Number of documents retrieved for query 2: 64\n",
            "Names of the documents retrieved for query 2: cranfield0049.txt, cranfield0054.txt, cranfield0061.txt, cranfield0071.txt, cranfield0074.txt, cranfield0076.txt, cranfield0082.txt, cranfield0088.txt, cranfield0094.txt, cranfield0101.txt, cranfield0165.txt, cranfield0168.txt, cranfield0171.txt, cranfield0177.txt, cranfield0178.txt, cranfield0189.txt, cranfield0206.txt, cranfield0211.txt, cranfield0218.txt, cranfield0219.txt, cranfield0233.txt, cranfield0240.txt, cranfield0261.txt, cranfield0327.txt, cranfield0328.txt, cranfield0344.txt, cranfield0351.txt, cranfield0364.txt, cranfield0365.txt, cranfield0366.txt, cranfield0370.txt, cranfield0387.txt, cranfield0397.txt, cranfield0417.txt, cranfield0435.txt, cranfield0440.txt, cranfield0474.txt, cranfield0491.txt, cranfield0495.txt, cranfield0534.txt, cranfield0538.txt, cranfield0547.txt, cranfield0557.txt, cranfield0569.txt, cranfield0689.txt, cranfield0774.txt, cranfield0797.txt, cranfield0943.txt, cranfield0961.txt, cranfield0963.txt, cranfield0967.txt, cranfield0978.txt, cranfield0985.txt, cranfield1061.txt, cranfield1109.txt, cranfield1216.txt, cranfield1235.txt, cranfield1241.txt, cranfield1242.txt, cranfield1300.txt, cranfield1326.txt, cranfield1375.txt, cranfield1383.txt, cranfield1386.txt\n",
            "Number of comparisons required for query 2: 59\n"
          ]
        }
      ]
    }
  ]
}